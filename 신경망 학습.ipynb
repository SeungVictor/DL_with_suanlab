{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyjyu4FzUAVw",
    "tags": []
   },
   "source": [
    "# 신경망 학습 with [suanlab](https://www.youtube.com/watch?v=kHXrjyqyfE4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7te43hqyiiJ",
    "tags": []
   },
   "source": [
    "### 필요한 모듈 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Qf2F_YbdybBE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tqdm.notebook import tqdm #시간재는 모듈\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 미분과 기울기\n",
    "\n",
    "- 어떤 함수를 나타내는 그래프에서 한 점의 미분값(미분계수)를 구하는 것은 해당 점에서의 접선을 의미\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/0/0f/Tangent_to_a_curve.svg/440px-Tangent_to_a_curve.svg.png)\n",
    "<br /><sub>출처: https://ko.wikipedia.org/wiki/%EB%AF%B8%EB%B6%84</sub>\n",
    "\n",
    "* 기울기는 **방향성**을 가짐\n",
    "  + 이용할 미분 식 (수치 미분)  \n",
    "      $\\frac{df(x)}{dx} = \\lim_{x \\to \\infty} \\frac{f(x+h) - f(x-h)}{2h}$\n",
    "\n",
    "* [주의] $ \\ h \\ $는 아주 작은 수를 뜻하는데, 예를 들어 $\\ 10e-50 \\ $ 정도의 수를 하면 파이썬은 이를 $0.0$으로 인식\n",
    "* 따라서, 딥러닝에서 아주 작은 수를 정할 때 $\\ 1e-4\\ $ 정도로 설정해도 무방"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y4OMFGrjyq1c"
   },
   "outputs": [],
   "source": [
    "def differential(f,x):\n",
    "    eps = 1e-5\n",
    "    diff_value = np.zeros_like(x) # x의크기를 0으로 채운것\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        temp_val = x[i]\n",
    "        \n",
    "        x[i] = temp_val + eps\n",
    "        f_h1 = f(x)\n",
    "        \n",
    "        x[i] = temp_val - eps\n",
    "        f_h2 = f(x)\n",
    "        \n",
    "        diff_value[i] = (f_h1 - f_h2) / (2 * eps)\n",
    "        x[i] =temp_val\n",
    "        \n",
    "    return diff_value\n",
    "\n",
    "\n",
    "# 2d 미분함수\n",
    "def differential_2d(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return differential(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = differential(f, x)\n",
    "        \n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sj3ETZY2W73s"
   },
   "source": [
    "## 하이퍼 파라미터(Hyper Parameter)\n",
    "- 사람이 직접 설정해야하는 매개변수\n",
    "- 학습이 되기전 미리 설정되어 상수취급\n",
    "  - 손실 함수 (Cost Function)\n",
    "  - 학습률 (Learning Rate)\n",
    "  - 학습 반복 횟수 (Epochs)\n",
    "  - 미니 배치 크기 (Batch Size)\n",
    "  - 은닉층의 노드 개수 (Units)\n",
    "  - 노이즈 (Noise)\n",
    "  - 규제화 (Regularization)\n",
    "  - 가중치 초기화 (Weights Initialization)\n",
    "\n",
    "- 신경망의 매개변수인 가중치는 학습 알고리즘에 의해 **자동**으로 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSVzmRVLJrwm",
    "tags": []
   },
   "source": [
    "# 신경망 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0F8W4x9kembv"
   },
   "source": [
    "## 퍼셉트론\n",
    "\n",
    "- 인공신경망의 한 종류\n",
    "- 다수의 입력($x_1, x_2, ..., x_n$)과 가중치($w_1, w_2, ..., w_n$)를 곱하여 그 값에 편향($bias$)을 더한 값이 어느 임계치 값($\\theta$)을 초과하면 활성화 함수를 통과한 출력값을 내보냄\n",
    "![perceptron](https://miro.medium.com/max/1400/1*ofVdu6L3BDbHyt1Ro8w07Q.png)\n",
    "<br /><sub>출처: https://towardsdatascience.com/rosenblatts-perceptron-the-very-first-neural-network-37a3ec09038a</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxALA-qreoJ-"
   },
   "source": [
    "## 뉴런의 수학적 표현\n",
    "\n",
    "![](https://cs231n.github.io/assets/nn1/neuron_model.jpeg)\n",
    "<br /><sub>출처: https://cs231n.github.io/convolutional-networks/</sub>\n",
    "\n",
    "$\\qquad y = f(\\sum_{i} w_ix_i + b) \\quad $\n",
    "\n",
    "  - $f\\ $ : 활성화 함수\n",
    "    - 임계값($\\theta$)을 경계로 출력이 바뀜\n",
    "\n",
    "  - $b\\ \\ $ :  편향\n",
    "    - <u>결정 경계선을 원점에서부터 벗어나게 해줌</u>\n",
    "    - 따로 표현이 없어도 기본적으로 존재한다고 생각\n",
    "\n",
    "  - $\\sum_{i} w_ix_i$ :$\\quad $두 벡터의 내적으로 표현 가능\n",
    "     \n",
    "     $\\\\ \\quad x_1w_1 + x_2w_2 +\\ ... \\ + x_nw_n = w^Tx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgdNIo5dfx2Y"
   },
   "source": [
    "## 활성화 함수(Activation Function)\n",
    "\n",
    "- 입력 신호의 총합을 출력 신호로 변환하는 함수\n",
    "- 활성화 함수에 따라 출력값이 결정\n",
    "- 단층, 다층 퍼셉트론 모두 사용\n",
    "- 대표적인 활성화 함수\n",
    "  - Sigmoid\n",
    "  - ReLU\n",
    "  - tanh \n",
    "  - Identity Function\n",
    "  - Softmax\n",
    "\n",
    "-  하나의 layer에서 다음 layer로 넘어갈 때는 항상 활성화 함수를 통과\n",
    "    \n",
    "- [참고] 여러가지 활성화 함수  \n",
    " https://en.wikipedia.org/wiki/Activation_function  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Sigmoid Function(시그모이드 함수)\n",
    "- 이진분류(binary classification)에 주로 사용\n",
    "  - 마지막 출력층의 활성화 함수로 사용\n",
    "- 출력값이 0~1 의 값이며, 이는 **확률**로 표현 가능\n",
    "\n",
    "\n",
    "$\\quad y = \\frac{1}{1 + e^{-x}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax\n",
    "\n",
    "- 다중 클래스 분류에 사용(Multi Class Classification)\n",
    "- 입력값의 영향을 크게 받음  \n",
    "  입력값이 크면 출력값도 큼\n",
    "- 출력값을 확률에 대응가능\n",
    "- 출력값의 **총합은 1**\n",
    "\n",
    "- 수식  \n",
    " ### $ y_k = \\frac{exp(a_k)}{\\sum_{i=1}{exp(a_i)}}$\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*670CdxchunD-yAuUWdI7Bw.png)\n",
    "<br /><sub>출처: https://medium.com/data-science-bootcamp/understand-the-softmax-function-in-minutes-f3a59641e86d</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWXjAqjngPi0",
    "tags": []
   },
   "source": [
    "\n",
    "### 하이퍼볼릭탄젠트 함수(Hyperbolic tangent function, tanh)\n",
    "\n",
    " ### $ \\quad y = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*pHjovxWB8BvI71ZkS-o_3A.png)\n",
    "<br /><sub>출처: https://medium.com/@toprak.mhmt</sub>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "iF3lGXSegNZJ"
   },
   "outputs": [],
   "source": [
    "def tahn(x):\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9q3FQagJgJDs",
    "tags": []
   },
   "source": [
    "\n",
    "### ReLU(Rectified Linear Unit)\n",
    "\n",
    "- 가장 많이 쓰이는 함수 중 하나  \n",
    "  \n",
    "  ### $ y = \\begin{cases}\n",
    "0 \\quad (x \\le 0) \\\\\n",
    "x \\quad (x > 0)\n",
    "\\end{cases} $\n",
    "\n",
    "![](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png)\n",
    "<br /><sub>출처: https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/</sub>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "2cEsh_FqgHUr"
   },
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    if x > 0:\n",
    "        return x\n",
    "    else :\n",
    "        return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MD3C4UcVgrxk",
    "tags": []
   },
   "source": [
    "- LeakyReLU\n",
    "\n",
    "### $ \\ \\ f_a(x) = \\begin{cases}\n",
    "x \\quad (x \\ge 0) \\\\\n",
    "ax \\quad (x < 0)\n",
    "\\end{cases}$ \n",
    "\n",
    "![](https://i0.wp.com/knowhowspot.com/wp-content/uploads/2019/04/IMG_20190406_220045-1.jpg)\n",
    "<br /><sub>출처: https://knowhowspot.com/technology/ai-and-machine-learning/artificial-neural-network-activation-function/</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BigOia-QgmcF"
   },
   "outputs": [],
   "source": [
    "def LeakyReLU(x):\n",
    "    a = 0.01\n",
    "    return np.maximum(a*x, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDh7qyImg4me"
   },
   "source": [
    "- ELU(Exponential Linear Units)  \n",
    "\n",
    "  $ f(\\alpha, x) = \\begin{cases}\n",
    "\\alpha \\ (e^x - 1) \\quad (x \\le 0) \\\\\n",
    "x \\qquad \\qquad (x > 0)\n",
    "\\end{cases}$  \n",
    "\n",
    "![](https://www.researchgate.net/publication/331794632/figure/fig1/AS:736888264609792@1552699261431/Exponential-Linear-Unit-activation-function-input-output-mapping-The-activation-function.jpg)\n",
    "<br /><sub>출처: https://www.researchgate.net/figure/Exponential-Linear-Unit-activation-function-input-output-mapping-The-activation-function_fig1_331794632</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1nImmUf9g2lV"
   },
   "outputs": [],
   "source": [
    "def ELU(x):\n",
    "    alpha = 1.0\n",
    "    return (x >= 0) * x + ( x < 0) * alpha * (np.exp(x)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실함수 (Loss Function, Cost Function)\n",
    "\n",
    "- 학습이 진행되면서 해당 과정이 얼마나 잘 되고 있는지 나타내는 지표\n",
    "- 손실 함수에 따른 결과를 통해 학습 파라미터를 조정\n",
    "- 최적화 이론에서 최소화 하고자 하는 함수\n",
    "- <u>**미분 가능한 함수 사용**</u>\n",
    "\n",
    "![](https://drive.google.com/uc?id=1SMxO-SrTzm8-Fq07T_rdkLnukxgPlmuL)\n",
    "<br /><sub>출처: https://zhuanlan.zhihu.com/p/85540935\\</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평균제곱오차(Mean Squared Error, MSE)\n",
    "- 가장 많이 쓰이는 손실 함수 중 하나\n",
    "\n",
    "- 오차가 커질수록 손실함수가 빠르게 증가\n",
    "  - 정답과 예측한 값의 차이가 클수록 더 많은 페널티를 부여\n",
    "\n",
    "- 회귀 (Regression)에 쓰임\n",
    "\n",
    "![](https://miro.medium.com/max/1152/1*EqTaoCB1NmJnsRYEezSACA.png)\n",
    "<br /><sub>출처: https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0</sub>\n",
    "\n",
    "### $ \\qquad \\qquad E = \\frac{1}{n}\\sum_{i=1}^n ( y_i - \\tilde{y}_i)^2 $\n",
    "\n",
    "  - $y_i$ : 학습 데이터의 $i\\ $번째 정답\n",
    "\n",
    "  - $\\tilde{y}_i$ : 학습 데이터의 입력으로 추정한 $i\\ $번째 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(pred_y, true_y):\n",
    "    return np.mean(np.sum(np.square((y - pred_y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교차 엔트로피 오차(Cross Entropy Error, CEE)\n",
    "\n",
    "- 이진 분류(Binary Classification), 다중 클래스 분류(Multi Class Classification)\n",
    "\n",
    "- 소프트맥스(softmax)와 원-핫 인코딩(ont-hot encoding) 사이의 출력 간 거리를 비교\n",
    "\n",
    "- 정답인 클래스에 대해서만 오차를 계산\n",
    "  - 정답을 맞추면 오차가 0, 틀리면 그 차이가 클수록 오차가 무한히 커짐\n",
    "\n",
    "### $ \\qquad \\qquad E = - \\frac{1}{N}\\sum_{n} \\sum_{i}  y_i\\ log\\tilde{y}_i  $ \n",
    "\n",
    "- $y_i$ : 학습 데이터의 $i\\ $번째 정답 (원-핫 인코딩, one-hot encoding)\u001c",
    "\n",
    "\n",
    "- $\\tilde{y}_i$ : 학습 데이터의 입력으로 추정한 $i\\ $번째 출력\n",
    "\n",
    "- $N$ : 전체 데이터의 개수\n",
    "\n",
    "- $i$ : 데이터 하나당 클래스 개수\n",
    "\n",
    "- $y = log(x)$는\n",
    "  - $x$가 0에 가까울 수록 $y$값은 무한히 커짐\n",
    "  \n",
    "  - 1에 가까울 수록 0에 가까워짐\n",
    "   \n",
    "![](https://mathcracker.com/images/legacy/log-graph.png)\n",
    "<br /><sub>출처: https://mathcracker.com/finding-the-log-graph</sub>\n",
    "\n",
    "- 정답 레이블($y_i$)은 원-핫 인코딩으로 정답인 인덱스에만 1이고, 나머지는 모두 0\n",
    "\n",
    "- 따라서, 위 수식은 다음과 같이 나타낼 수 있음\n",
    "\n",
    "### $ \\qquad \\qquad E = - log\\tilde{y}_i  $\n",
    "\n",
    "  - 소프트맥스를 통해 나온 신경망 출력이 0.6이라면 $\\ -log0.6 \\fallingdotseq -0.51\\ $이 되고,  \n",
    "    신경망 출력이 0.3이라면  $\\ -log0.3 \\fallingdotseq -1.2\\ $이 됨\n",
    "\n",
    "  - 정답에 가까워질수록 오차값은 작아짐\n",
    "  \n",
    "  - 학습시, **원-핫 인코딩에 의해 정답 인덱스만 살아 남아 비교하지만, 정답이 아닌 인덱스들도 학습에 영향을 미침**  \n",
    "    <u>다중 클래스 분류는 소프트맥스(softmax) 함수를 통해 전체 항들을 모두 다루기 때문</u>\n",
    "\n",
    "![](https://miro.medium.com/max/836/1*T8KWtAn8FkAcsg8RsjiZ6Q.png)\n",
    "<br /><sub>출처: https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(pred_y, true_y):\n",
    "    if true_y.ndim == 1:\n",
    "        true_y = true_y.reshape(1, -1)\n",
    "        pred_y = pred_y.reshape(1, -1)\n",
    "        \n",
    "    delta = 1e-7\n",
    "    return -np.sum(true_y * np.log(pred_y +delta))\n",
    "\n",
    "def cross_entropy_error_for_batch(pred_y, true_y):\n",
    "    if true_y.ndim == 1:\n",
    "        true_y = true_y.reshape(1, -1)\n",
    "        pred_y = pred_y.reshape(1, -1)\n",
    "        \n",
    "    delta = 1e-7\n",
    "    batch_size = pred_y.shape[0] #shape 크기만큼 배치가 나누어짐\n",
    "    return -np.sum(true_y * np.log(pred_y +delta)) / batch_size #결과값을 배치로 나눔"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이진 분류에서의 교차 크로스 엔트로피(Binary Cross Entropy, BCE)\n",
    "\n",
    "- 이진 분류 문제(Binary Classification Problem)에서도 크로스 엔트로피 오차를 손실함수로 사용 가능\n",
    "\n",
    "### $ \\qquad \\qquad E = - \\sum_{i=1}^2  y_i\\ log\\tilde{y}_i \\\\ \n",
    "\\qquad \\qquad \\ \\ \\ = -y_1\\ log\\ \\tilde{y}_1 - (1 - y_1)log(1-\\ \\tilde{y}_1) $  \n",
    "$\\qquad \\qquad \\qquad ( \\because y_2 = 1 - y_1)$\n",
    "\n",
    "- $y_i$ : 학습 데이터의 $i\\ $번째 정답 (원-핫 인코딩, one-hot encoding)\u001c",
    "\n",
    "\n",
    "- $\\tilde{y}_i$ : 학습 데이터의 입력으로 추정한 $i\\ $번째 출력\n",
    "\n",
    "\n",
    "- 2개의 클래스를 분류하는 문제에서  \n",
    "  1번이 정답일 확률이 0.8이고, 실제로 정답이 맞다면 위 식은 다음과 같이 나타낼 수 있음\n",
    "\n",
    "### $ \\qquad \\qquad E = -y_1\\ log\\ \\tilde{y}_1 - (1 - y_1)\\ log\\ (1-\\ \\tilde{y}_1) \\\\\n",
    "\\qquad \\qquad \\ \\ \\ = -1\\ log\\ 0.8 - (1 - 1)\\ log\\ (1 - 0.8)\\\\ \n",
    "\\qquad \\qquad \\ \\ \\ = -log\\ 0.8 \\\\\n",
    "\\qquad \\qquad \\ \\ \\ \\fallingdotseq -0.22\n",
    "$\n",
    "\n",
    "- 반대로, 실제로 정답이 2번이었다면, 식은 다음과 같이 나타낼 수 있음\n",
    "\n",
    "### $ \\qquad \\qquad E = -y_1\\ log\\ \\tilde{y}_1 - (1 - y_1)\\ log\\ (1-\\ \\tilde{y}_1) \\\\\n",
    "\\qquad \\qquad \\ \\ \\ = -0\\ log\\ 0.8 - (1 - 0)\\ log\\ (1 - 0.8)\\\\ \n",
    "\\qquad \\qquad \\ \\ \\ = -log\\ 0.2 \\\\\n",
    "\\qquad \\qquad \\ \\ \\ \\fallingdotseq -1.61\n",
    "$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error_for_bin(pred_y, true_y):\n",
    "    return 0.5 * np.sum((-true_y * np.log(pred_y) - (1 - true_y) * np.log(1 -pred_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1IuDL8R7wrx",
    "tags": []
   },
   "source": [
    "## 다중 클래스 분류 : MNIST Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CiJ5Gmq9Wpa"
   },
   "source": [
    "### 배치 처리\n",
    "- 학습 데이터 전체를 한번에 진행하지 않고  \n",
    "  일부 데이터(샘플)을 확률적으로 구해서 조금씩 나누어 진행\n",
    "\n",
    "- 확률적 경사 하강법(Stochastic Gradient Descent) 또는  \n",
    "  미니 배치 학습법(mini-batch learning)이라고도 부름"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YUDNWwj49byH",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 신경망 구현 : MNIST "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDvtEiD77_gu"
   },
   "source": [
    "#### 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4WL7zXMl_uo9"
   },
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_rNg5Jn8FRA"
   },
   "source": [
    "#### 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "u4wpsQGA8BOO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pU7nvkHO8IFR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = x_train[0]\n",
    "print(img.shape)\n",
    "\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](https://wikidocs.net/images/page/64066/conv2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WbBA1Kl18KGT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTFu8i-z8U_C"
   },
   "source": [
    "#### 데이터 전처리 (Data Preprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "다차원 배열을 1차원으로 바꾸는 함수 <br>\n",
    "ravel(), reshape(), flatten() <br>\n",
    "https://m.blog.naver.com/wideeyed/221533365486"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "q76pjKDVftHJ"
   },
   "outputs": [],
   "source": [
    "def flatten_for_mnist(x):\n",
    "    temp = np.zeros((x.shape[0], x[0].size)) #현재 데이터 사이즈대로 0으로채워진 temp생성\n",
    "    \n",
    "    for idx, data in enumerate(x):\n",
    "        temp[idx, :] = data.flatten() # 0으로채워진 temp에 idx 및 전체범위에 1차원으로바꾼 data를 집어넣음 \n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XswAi5IS-rqU"
   },
   "source": [
    "### 원-핫 인코딩(one-hot encoding)\n",
    "- 범주형 변수를 표현할 때 사용\n",
    "- 가변수(Dummy Variable)이라고도 함\n",
    "- 정답인 레이블을 제외하고 0으로 처리  \n",
    "https://wikidocs.net/22647\n",
    "\n",
    "![](https://miro.medium.com/max/1400/0*T5jaa2othYfXZX9W.)\n",
    "<sub>출처: https://medium.com/@michaeldelsole/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179</sub>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vvMWrDOR8Mns"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n",
      "Metal device set to: Apple M1\n",
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-14 00:43:21.945365: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-10-14 00:43:21.945456: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# 0~255 256개색표현하므로 255(n-1)으로 나눠서 정규화 ->가장큰건 1 가장작은건 0으로 표현됨 \n",
    "x_train, x_test = x_train / 255.0, x_test/ 255.0\n",
    "\n",
    "\n",
    "# 3차원(28 * 28 * 1)을 -> 1차원(784)로 전환\n",
    "x_train = flatten_for_mnist(x_train)\n",
    "x_test = flatten_for_mnist(x_test)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# y를 one-hot-encoding  depth->원핫의 스칼라수를 결정 \n",
    "# 정답이 0~9이므로 depth는 10\n",
    "y_train_ohe = tf.one_hot(y_train, depth=10).numpy()\n",
    "y_test_ohe = tf.one_hot(y_test, depth=10).numpy()\n",
    "\n",
    "print(y_train_ohe.shape)\n",
    "print(y_test_ohe.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9LjpWz0dotJs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.0\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0].max(), x_train[0].min())\n",
    "print(y_train_ohe[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333333 0.68627451 0.10196078 0.65098039 1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117647 0.36862745 0.60392157\n",
      " 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215686\n",
      " 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.98431373 0.36470588 0.32156863\n",
      " 0.32156863 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882353 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.31372549 0.61176471 0.41960784 0.99215686\n",
      " 0.99215686 0.80392157 0.04313725 0.         0.16862745 0.60392157\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509804 0.99215686 0.74509804 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313725\n",
      " 0.74509804 0.99215686 0.2745098  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.1372549  0.94509804\n",
      " 0.88235294 0.62745098 0.42352941 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764706 0.94117647 0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.17647059 0.72941176 0.99215686 0.99215686\n",
      " 0.58823529 0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.36470588 0.98823529 0.99215686 0.73333333\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.97647059 0.99215686 0.97647059 0.25098039 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980392 0.71764706 0.99215686\n",
      " 0.99215686 0.81176471 0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.58039216\n",
      " 0.89803922 0.99215686 0.99215686 0.99215686 0.98039216 0.71372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.78823529 0.30588235 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882353 0.83529412 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058824\n",
      " 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588\n",
      " 0.31372549 0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333333 0.99215686\n",
      " 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5GUaa92Y9RhY"
   },
   "source": [
    "#### 하이퍼 파라미터(Hyper Parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "sk3FXXLi9Th5"
   },
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "lr = 0.1\n",
    "batch_size = 100\n",
    "train_size = x_train.shape[0] # 784\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSoV9fyj8_u7"
   },
   "source": [
    "#### 2층 신경망으로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "XBObD5Fw89HI"
   },
   "outputs": [],
   "source": [
    "class MyModel():\n",
    "    \n",
    "    def __init__(self):\n",
    "                \n",
    "        def weight_init(input_nodes, hidden_nodes, output_units):\n",
    "            np.random.seed(777)\n",
    "            \n",
    "            params = {}\n",
    "            params['w_1'] = 0.01 * np.random.randn(input_nodes, hidden_nodes) #0.01 파라미터임 \n",
    "            params['b_1'] = np.zeros(hidden_nodes)\n",
    "            params['w_2'] = 0.01 * np.random.randn(hidden_nodes, output_units)\n",
    "            params['b_2'] = np.zeros(output_units)\n",
    "            \n",
    "            return params\n",
    "        \n",
    "        self.params = weight_init(784, 64, 10) \n",
    "        # 784 -> x flatten한값 \n",
    "        # 64 -> hidden nodes 파라미터 지정한것 \n",
    "        #output_units mnist결과값 숫자 0~9이므로 10개 \n",
    "        \n",
    "        \n",
    "    def predict(self, x):\n",
    "        W_1, W_2 = self.params['w_1'], self.params['w_2']\n",
    "        B_1, B_2 = self.params['b_1'], self.params['b_2']\n",
    "        \n",
    "        A1 = np.dot(x, W_1) + B_1\n",
    "        Z1 = sigmoid(A1)\n",
    "        A2 = np.dot(Z1, W_2) + B_2\n",
    "        pred_y = softmax(A2) #Multiclass classification이므로 마지막에 softmax사용한것 \n",
    "        \n",
    "        return pred_y\n",
    "    \n",
    "    def loss(self, x, true_y):\n",
    "        pred_y = self.predict(x)\n",
    "        return cross_entropy_error_for_bin(pred_y, true_y)\n",
    "    \n",
    "    # classification 정확도 계산\n",
    "    def accuracy(self, x, true_y):\n",
    "        pred_y = self.predict(x) #현재 예측된 pred_y값\n",
    "        y_argmax = np.argmax(pred_y, axis=1) # 예측한값의 인덱스를 반환 \n",
    "        t_argmax = np.argmax(true_y, axis=1) # 실제값의 인덱스를 반환 \n",
    "        \n",
    "        accuracy = np.sum(y_argmax == t_argmax) / float(x.shape[0]) \n",
    "        # 예측한것과 실제값의 인덱스가 같은것-> 맞춘것이므로  shape크기만큼 나눠주면 ->정확도\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def get_gradient(self, x, t):\n",
    "        \n",
    "        def loss_grad(grad):\n",
    "            return self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['w_1'] = differential_2d(loss_grad, self.params['w_1'])\n",
    "        grads['b_1'] = differential_2d(loss_grad, self.params['b_1'])\n",
    "        grads['w_2'] = differential_2d(loss_grad, self.params['w_2'])\n",
    "        grads['b_2'] = differential_2d(loss_grad, self.params['b_2'])\n",
    "        \n",
    "        return grads\n",
    "                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maKNIlK-xJ5k"
   },
   "source": [
    "#### 모델 생성 및 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[batch size](https://losskatsu.github.io/machine-learning/epoch-batch/?fbclid=IwAR1xKJhQwrye0_2JfYBIkq6mQAYI7pq7IkBjesr2YMdkpUMd6-nYPryC7fU#1-%EC%82%AC%EC%A0%84%EC%A0%81-%EC%9D%98%EB%AF%B8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "XSEARgNIop8t"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3568d4237b0477da5dd0227d3347ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Cost465.9060502398182, Train Acc: 0.10441666666666667, Test Acc: 0.1028\n",
      "Epoch: 2, Cost361.84409502611004, Train Acc: 0.09751666666666667, Test Acc: 0.0974\n",
      "총학습소요시간 : 289.785s\n"
     ]
    }
   ],
   "source": [
    "model = MyModel()\n",
    "train_loss_list = list()\n",
    "train_acc_list = list()\n",
    "test_acc_list = list()\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "start_time = time.time()\n",
    "for i in tqdm(range(epochs)):\n",
    "    \n",
    "    #batch인덱스를 만들어서 모델링 \n",
    "    batch_idx = np.random.choice(train_size, batch_size) #train size 784 batch size 100\n",
    "    x_batch = x_train[batch_idx]\n",
    "    y_batch = y_train_ohe[batch_idx]\n",
    "    \n",
    "    grads = model.get_gradient(x_batch, y_batch)\n",
    "    \n",
    "    for key in grads.keys():\n",
    "        model.params[key] -= lr * grads[key] \n",
    "    #기울기계산한것의 key값을 가져와서 lr을 곱한것으로 model의 params(weights)를 업뎃 \n",
    "    #즉 x값과 초기 가중치를 64개, 2layer의 히든노드에 각 활성화함수(sigmoid, softmax)를 거쳐서 나온값 \n",
    "    #y_hat과 y의 오차를 loss함수(cross entropy)로 구함 그거에 대한 미분값이 0이되는것이 예측을 정확하게 하는것이므로 \n",
    "    # 그 미분값에 learning rate를 곱한것을 다시 weight parameter에 업데이트 해줌 1순환 한것 -> 1 epoch \n",
    "    # epoch를 반복할수록 오차가 줄어들고 정확도가 높아지는것이 신경망의 원리\n",
    "    # 손실함수를 학습 파라미터(가중치, 편향)로 미분하여  \n",
    "    # 마지막 layer로부터 앞으로 하나씩 연쇄법칙을 이용하여 미분\n",
    "    # 각 layer를 통과할 때마다 저장된 값을 이용 backward하면서 error 전달 파라미터 갱신하는방법 -> 오차역전파 backpropagation \n",
    "    \n",
    "    \n",
    "        \n",
    "    loss = model.loss(x_batch, y_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    train_accuracy = model.accuracy(x_train, y_train_ohe)\n",
    "    test_accuracy = model.accuracy(x_test, y_test_ohe)\n",
    "    train_acc_list.append(train_accuracy)\n",
    "    test_acc_list.append(test_accuracy)\n",
    "    \n",
    "    print(\"Epoch: {}, Cost{}, Train Acc: {}, Test Acc: {}\".format(i+1, loss, train_accuracy, test_accuracy))\n",
    "          \n",
    "end_time = time.time()\n",
    "          \n",
    "print(\"총학습소요시간 : {:.3f}s\".format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of _7 신경망 학습",
   "provenance": [
    {
     "file_id": "1hk4I8R9TidWzFQrKTJjuij54kN_QC3jY",
     "timestamp": 1628856784696
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
